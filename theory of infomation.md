# theory of infomation
define X:sample x:random variable in X . then ,
# information entropy 
H(X)=-∑p(x)*log p(x)  x∈X    A measurement for nondeterminacy. the bigger information entroy is,the nondeterminacy.
# combination entroy
H(X,Y)=-∑p(x,y)*log p(x,y) x∈X y∈Y describe the nondeterminacy of a caple of random variable(x,y).
# conditional entroy
H(Y|X)=-∑p(x,y)log p(y|x) x∈X y∈Y when a random variable(x) is certain,the nondeterminacy of another random variable(y).
# relative entroy(information gain,KL distance)
